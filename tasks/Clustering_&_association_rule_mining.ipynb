{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task\n",
    "The task is split into two sub-tasks: finding optimal hub locations and finding interesting relationships between product groups.\n",
    "\n",
    "### Part 1: Finding optimal hub locations\n",
    "Guidelines for the Analysis phase:\n",
    "\n",
    "Visualize the client locations by making a two-dimensional scatterplot. Can you give a geographic interpretation for what you see? Using k-means clustering, find optimal locations (i.e. x and y coordinates) for three drone depots. Each depot should serve its surrounding clients.\n",
    "Hint: you can use Seaborn https://seaborn.pydata.org/generated/seaborn.scatterplot.html or Matplotlib: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html.\n",
    "\n",
    "Hint: The centroids serve as the depot locations. You will later need to change the number of depots, so design your program in such a way that you just need to modify a single value to do that.\n",
    "\n",
    "Attach the information on the closest depot to each client. That is, generate a data frame that is similar to the original one with the exception that it has an additional column that contains the identifier of the depot nearest to the client. Print the first 10 rows of the new data frame.\n",
    "\n",
    "Make a scatterplot that uses three different colours. The markers with the same colour are served by the same depot. Hint: Re-check the web page(s) mentioned in the first task.\n",
    "\n",
    "Play with the number of depots. What are the optimal locations for 10 depots, for example? Do you see a difference in the computation time when the number of depots increases?\n",
    "\n",
    "Replace k-means with agglomerative hierarchical clustering and explore it with various depot numbers. What are your observations?\n",
    "\n",
    "In the end, your report should give a recommendation on how the depots should be placed, depending on the number of depots. You should also discuss the differences between k-means and hierarchical clustering in this context.\n",
    "\n",
    "### Part 2: Finding interesting relationships between product groups\n",
    "Use association rule mining to find interesting relationships between product groups.\n",
    "\n",
    "Your report should include a clear recommendation on how the company should use the results of the association rule mining to increase its revenue."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Business Understanding\n",
    "### The Goal:\n",
    "The goal of this task is to determine the most efficient locations for drone depots to serve clients. By analyzing the geographic distribution of client locations. The aim is to find the optimal locations for the depots to minimize the distance between the depots and the clients they serve. The task is split into two sub-tasks: finding optimal hub locations and finding interesting relationships between product groups.\n",
    "First part will be achieved by using k-means clustering to find the optimal locations for three drone depots. The second part will be achieved by using association rule mining to find interesting relationships between product groups.\n",
    "\n",
    "### Requirements & Limitations:\n",
    "- **Visualize client locations**: Create a two-dimensional scatterplot of client locations.\n",
    "- **K-means clustering**: Use k-means clustering to find optimal locations for three drone depots.\n",
    "- **Depot assignment**: Attach the closest depot information to each client in a new data frame.\n",
    "- **Scatterplot with depot assignments**: Create a scatterplot with different colors for each depot's clients.\n",
    "- **Hierarchical clustering**: Replace k-means with agglomerative hierarchical clustering and explore with various depot numbers.\n",
    "- **Association rule mining**: Use association rule mining to find relationships between product groups.\n",
    "\n",
    "### Expected Outcome:\n",
    "- **Scatterplot of client locations**: A visual representation of client locations.\n",
    "- **Optimal depot locations**: Coordinates of the optimal depot locations using k-means clustering.\n",
    "- **Hierarchical clustering analysis**: Observations and comparisons of depot placements using hierarchical clustering.\n",
    "- **Recommendations**: A report with recommendations on depot placements and a discussion on clustering methods."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Understanding\n",
    "### Dataset drone_prod_groups.csv\n",
    "This dataset contains information about various product groups. Each row represents a unique product group with multiple products listed under it.\n",
    "- **Columns**:\n",
    "ID: Identifier for the product group.\n",
    "Prod1 to Prod20: Products associated with the product group.\n",
    "\n",
    "### Dataset drone_cust_locations.csv\n",
    "This dataset contains the geographic locations of clients. Each row represents a client with their respective coordinates.\n",
    "- **Columns**:\n",
    "clientid: Unique identifier for the client.\n",
    "x: X-coordinate of the client's location.\n",
    "y: Y-coordinate of the client's location."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          clientid            x            y\n",
      "count  5956.000000  5956.000000  5956.000000\n",
      "mean   2978.500000   508.823177   427.554772\n",
      "std    1719.493433   271.061462   289.044640\n",
      "min       1.000000     0.017692     0.043285\n",
      "25%    1489.750000   282.582920   170.079921\n",
      "50%    2978.500000   518.100892   397.786441\n",
      "75%    4467.250000   727.156497   669.982518\n",
      "max    5956.000000   999.533215   999.731720\n",
      "                  ID          Prod1         Prod2          Prod3  \\\n",
      "count  100000.000000  100000.000000  100000.00000  100000.000000   \n",
      "mean    50000.500000       0.109980       0.13098       0.032710   \n",
      "std     28867.657797       0.312866       0.33738       0.177877   \n",
      "min         1.000000       0.000000       0.00000       0.000000   \n",
      "25%     25000.750000       0.000000       0.00000       0.000000   \n",
      "50%     50000.500000       0.000000       0.00000       0.000000   \n",
      "75%     75000.250000       0.000000       0.00000       0.000000   \n",
      "max    100000.000000       1.000000       1.00000       1.000000   \n",
      "\n",
      "               Prod4          Prod5         Prod6          Prod7  \\\n",
      "count  100000.000000  100000.000000  100000.00000  100000.000000   \n",
      "mean        0.035850       0.104590       0.02952       0.134990   \n",
      "std         0.185917       0.306026       0.16926       0.341715   \n",
      "min         0.000000       0.000000       0.00000       0.000000   \n",
      "25%         0.000000       0.000000       0.00000       0.000000   \n",
      "50%         0.000000       0.000000       0.00000       0.000000   \n",
      "75%         0.000000       0.000000       0.00000       0.000000   \n",
      "max         1.000000       1.000000       1.00000       1.000000   \n",
      "\n",
      "              Prod8          Prod9  ...         Prod11         Prod12  \\\n",
      "count  100000.00000  100000.000000  ...  100000.000000  100000.000000   \n",
      "mean        0.16179       0.198530  ...       0.108480       0.159710   \n",
      "std         0.36826       0.398895  ...       0.310987       0.366339   \n",
      "min         0.00000       0.000000  ...       0.000000       0.000000   \n",
      "25%         0.00000       0.000000  ...       0.000000       0.000000   \n",
      "50%         0.00000       0.000000  ...       0.000000       0.000000   \n",
      "75%         0.00000       0.000000  ...       0.000000       0.000000   \n",
      "max         1.00000       1.000000  ...       1.000000       1.000000   \n",
      "\n",
      "              Prod13         Prod14         Prod15         Prod16  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        0.013390       0.145570       0.118800       0.131000   \n",
      "std         0.114938       0.352676       0.323555       0.337402   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "             Prod17         Prod18         Prod19         Prod20  \n",
      "count  100000.00000  100000.000000  100000.000000  100000.000000  \n",
      "mean        0.05618       0.121660       0.206260       0.147980  \n",
      "std         0.23027       0.326894       0.404621       0.355082  \n",
      "min         0.00000       0.000000       0.000000       0.000000  \n",
      "25%         0.00000       0.000000       0.000000       0.000000  \n",
      "50%         0.00000       0.000000       0.000000       0.000000  \n",
      "75%         0.00000       0.000000       0.000000       0.000000  \n",
      "max         1.00000       1.000000       1.000000       1.000000  \n",
      "\n",
      "[8 rows x 21 columns]\n",
      "Index(['clientid', 'x', 'y'], dtype='object')\n",
      "Index(['ID', 'Prod1', ' Prod2', ' Prod3', ' Prod4', ' Prod5', ' Prod6',\n",
      "       ' Prod7', ' Prod8', ' Prod9', ' Prod10', ' Prod11', ' Prod12',\n",
      "       ' Prod13', ' Prod14', ' Prod15', ' Prod16', ' Prod17', ' Prod18',\n",
      "       ' Prod19', ' Prod20'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "locations_df = pd.read_csv('../datasets/drone_cust_locations.csv', delimiter=';')\n",
    "groups_df = pd.read_csv('../datasets/drone_prod_groups.csv')\n",
    "\n",
    "# Describe the data\n",
    "print(locations_df.describe())\n",
    "print(groups_df.describe())\n",
    "\n",
    "# Print data types\n",
    "print(locations_df.columns)\n",
    "print(groups_df.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation\n",
    "### Cleaning the Data:\n",
    "- Load the datasets: Read the drone_prod_groups.csv and drone_cust_locations.csv files into pandas DataFrames.\n",
    "- Handle missing values: Check for and handle any missing values in the datasets.\n",
    "\n",
    "### Feature selection:\n",
    "- Select relevant features: For drone_cust_locations, use clientid, x, and y. For drone_prod_groups, use ID and Prod1 to Prod20\n",
    "\n",
    "### Splitting the Data:\n",
    "- Split the data: For clustering, use the x and y coordinates from drone_cust_locations. For association rule mining, use the product columns from drone_prod_groups.\n",
    "\n",
    "### Data standardization:\n",
    "- Standardize the data: Normalize the x and y coordinates for clustering.\n",
    "- Convert data types: Ensure that the data types are appropriate for the analysis.\n",
    "\n",
    "### Prepare data for association rule mining:\n",
    "- Convert the product data to a boolean type.\n",
    "\n",
    "### Check for duplicate entries:\n",
    "- Check for duplicate client entries in the locations data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clientid    0\n",
      "x           0\n",
      "y           0\n",
      "dtype: int64\n",
      "ID         0\n",
      "Prod1      0\n",
      " Prod2     0\n",
      " Prod3     0\n",
      " Prod4     0\n",
      " Prod5     0\n",
      " Prod6     0\n",
      " Prod7     0\n",
      " Prod8     0\n",
      " Prod9     0\n",
      " Prod10    0\n",
      " Prod11    0\n",
      " Prod12    0\n",
      " Prod13    0\n",
      " Prod14    0\n",
      " Prod15    0\n",
      " Prod16    0\n",
      " Prod17    0\n",
      " Prod18    0\n",
      " Prod19    0\n",
      " Prod20    0\n",
      "dtype: int64\n",
      "Duplicate clients: 0\n",
      "Mean: [1.37193443e-16 3.81755667e-17], Std: [1. 1.]\n",
      "  antecedents consequents  antecedent support  consequent support  support  \\\n",
      "0    ( Prod9)    ( Prod2)             0.19853             0.13098  0.03210   \n",
      "1    ( Prod2)    ( Prod9)             0.13098             0.19853  0.03210   \n",
      "2   ( Prod19)    ( Prod2)             0.20626             0.13098  0.03346   \n",
      "3    ( Prod2)   ( Prod19)             0.13098             0.20626  0.03346   \n",
      "4   ( Prod12)    ( Prod5)             0.15971             0.10459  0.06683   \n",
      "\n",
      "   confidence      lift  representativity  leverage  conviction  \\\n",
      "0    0.161688  1.234451               1.0  0.006097    1.036631   \n",
      "1    0.245076  1.234451               1.0  0.006097    1.061656   \n",
      "2    0.162222  1.238528               1.0  0.006444    1.037292   \n",
      "3    0.255459  1.238528               1.0  0.006444    1.066079   \n",
      "4    0.418446  4.000822               1.0  0.050126    1.539685   \n",
      "\n",
      "   zhangs_metric   jaccard  certainty  kulczynski  \n",
      "0       0.236969  0.107932   0.035337    0.203382  \n",
      "1       0.218549  0.107932   0.058075    0.203382  \n",
      "2       0.242636  0.110146   0.035951    0.208841  \n",
      "3       0.221618  0.110146   0.061984    0.208841  \n",
      "4       0.892610  0.338431   0.350516    0.528709  \n",
      "[[ 0.42041374 -0.9089226 ]\n",
      " [-0.34115378  0.70112311]\n",
      " [-0.79725941  0.48362839]\n",
      " [ 0.8426402  -0.90418918]\n",
      " [ 0.1167811   0.88352777]]\n",
      "   Prod1   Prod2   Prod3   Prod4   Prod5   Prod6   Prod7   Prod8   Prod9  \\\n",
      "0  False   False   False   False   False   False   False   False    True   \n",
      "1  False    True   False   False   False   False   False   False    True   \n",
      "2  False   False   False   False   False   False    True   False   False   \n",
      "3   True   False   False    True   False   False   False   False   False   \n",
      "4  False   False   False   False   False   False   False   False    True   \n",
      "\n",
      "    Prod10   Prod11   Prod12   Prod13   Prod14   Prod15   Prod16   Prod17  \\\n",
      "0    False    False    False    False    False     True    False    False   \n",
      "1    False    False    False    False    False     True     True     True   \n",
      "2    False    False    False    False    False    False    False    False   \n",
      "3    False     True    False    False    False    False    False    False   \n",
      "4    False    False    False    False    False     True    False    False   \n",
      "\n",
      "    Prod18   Prod19   Prod20  \n",
      "0    False    False     True  \n",
      "1     True     True     True  \n",
      "2    False     True     True  \n",
      "3    False     True     True  \n",
      "4    False     True     True  \n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(locations_df.isnull().sum())\n",
    "print(groups_df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values (if any)\n",
    "locations_df.dropna(inplace=True)\n",
    "groups_df.dropna(inplace=True)\n",
    "\n",
    "# Select relevant features\n",
    "locations_df = locations_df[['clientid', 'x', 'y']]\n",
    "groups_df = groups_df.loc[:, ['ID'] + [col for col in groups_df.columns if 'Prod' in col]]\n",
    "\n",
    "# Splitting the Data\n",
    "X = locations_df[['x', 'y']]  # For clustering\n",
    "Y = groups_df.drop(columns=['ID'])  # For association rule mining\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert data to boolean type (fixes deprecation warning)\n",
    "Y = (Y > 0).astype(bool)\n",
    "\n",
    "# Check for duplicate client entries\n",
    "print(\"Duplicate clients:\", locations_df.duplicated(subset=['clientid']).sum())\n",
    "\n",
    "# Ensure standardization has no anomalies\n",
    "print(f\"Mean: {X_scaled.mean(axis=0)}, Std: {X_scaled.std(axis=0)}\")\n",
    "\n",
    "# Prepare data for association rule mining\n",
    "frequent_itemsets = apriori(Y, min_support=0.03, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "# Display results\n",
    "print(rules.head())\n",
    "\n",
    "# Print first few rows of prepared data\n",
    "print(X_scaled[:5])\n",
    "print(Y.head())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
